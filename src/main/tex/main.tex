%! Author = Andrew Selvia
%! Date = 2022.4.26

\documentclass[conference]{./IEEEtran/IEEEtran} % https://www.ctan.org/pkg/ieeetran
\usepackage{cite}
\usepackage[hidelinks]{hyperref}
\usepackage{csquotes} % https://tex.stackexchange.com/a/36813
\usepackage{mathtools} % https://tex.stackexchange.com/a/96353
\usepackage{amsmath}
\usepackage{amsfonts}

\begin{document}

    \title{DrQ-v2 Ablation Study}

    \author{
        \IEEEauthorblockN{Paul Mello}
        \IEEEauthorblockA{\textit{Department of Artificial Intelligence} \\
        \textit{San José State University}\\
        San José, California \\
        paul.mello@sjsu.edu}
        \and
        \IEEEauthorblockN{Andrew Selvia}
        \IEEEauthorblockA{\textit{Department of Software Engineering} \\
        \textit{San José State University}\\
        San José, California \\
        andrew.selvia@sjsu.edu}
        \and
        \IEEEauthorblockN{Hyelim Yang}
        \IEEEauthorblockA{\textit{Department of Artificial Intelligence} \\
        \textit{San José State University}\\
        San José, California \\
        hyelim.yang@sjsu.edu}
    }

    \maketitle

    \begin{abstract}
Reinforcement learning (RL) problems are notorious for their extensive training times due, in part, to uncertainty in the environment space. DrQ-v2 is an off-policy algorithm published by facebook in 2021 which efficiently models continuous systems through sampling as a means of speeding up convergence and learning. Uniquely, DrQ-v2 is the first RL algorithm to learn from direct pixel observation by comparing the current frame to the previous and future frames.
The purpose of this paper is thus to conduct an ablation study to understand the inner mechanisms of DrQ-v2 and attempt to improve its learning capabilities. In order to train our models we use MuJoCo environments to simulate continuous action spaces. Due to hardware limitations we train our models in environments of medium difficulty such as ___, ___, and ___. After tuning relevant hyperparameters, including ____ and ____, we find that DrQ-v2 is able to …. Describe results and strength of model through adjustments of hyper parameters.


    \end{abstract}

    \begin{IEEEkeywords}
        reinforcement learning, actor-critic, PPO
    \end{IEEEkeywords}

    \section{Introduction}\label{sec:introduction}
    Example citation: DrQ-v2 paper~\cite{yarats2021drqv2}.

    Example citation: original paper~\cite{yarats2021image}.

    Example quotation: \enquote{This is important.}

    Example inline math with vector notation: \(\mathbf{w} = [1, 2, 3]\).

    Example math block:

    \[w = w - \eta \nabla f(w).\]

    Reinforcement learning is plagued by ambiguity in the environment. As the environment grows in size or the potential hazards increase, a model must be capable of understanding how it will navigate the environment in a robust manner. Traditionally, RL has incurred immense computational challenges and has been very difficult to train. Modern RL models combat this by using dynamic systems. These dynamic systems can often be implemented using simple architectures such as the one proposed by Facebook researchers, DrQ-v2. DrQ-v2 shatters many of these expectations by significantly reducing training times and improving the ease of implementation.
    At its core DrQ-v2 is an off-policy sampling algorithm which uses many statistical principles to improve training time and convergence. (Discuss core of DrQ-v2 and why its fast)

    (Discuss what the aim of this paper is for a paragraph. Include why this is important and what we are trying to do.)


    \section{Problem Definition}\label{sec:problem-definition}

    \section{Methodology}\label{sec:methodology}

    TODO

    \section{Experiments}\label{sec:experiments}

    TODO

    \section{Conclusion}\label{sec:conclusion}

    TODO

    \bibliography{main}
    \bibliographystyle{ieeetr}
\end{document}
