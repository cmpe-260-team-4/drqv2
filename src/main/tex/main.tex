%! Author = Andrew Selvia
%! Date = 2022.4.26

\documentclass[conference]{./IEEEtran/IEEEtran} % https://www.ctan.org/pkg/ieeetran
\usepackage{cite}
\usepackage[hidelinks]{hyperref}
\usepackage{csquotes} % https://tex.stackexchange.com/a/36813
\usepackage{mathtools} % https://tex.stackexchange.com/a/96353
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{tikz} % To generate the plot from csv

\usepackage{pgfplots}
\pgfplotsset{width = 10cm, compat = 1.9}

%% We will externalize the figures
%\usepgfplotslibrary{external}
%\tikzexternalize

\usepackage{filecontents}

\begin{document}

    \title{DrQ-v2 Ablation Study}

    \author{
        \IEEEauthorblockN{Paul Mello}
        \IEEEauthorblockA{\textit{Department of Artificial Intelligence} \\
        \textit{San José State University}\\
        San José, California \\
        paul.mello@sjsu.edu}
        \and
        \IEEEauthorblockN{Andrew Selvia}
        \IEEEauthorblockA{\textit{Department of Software Engineering} \\
        \textit{San José State University}\\
        San José, California \\
        andrew.selvia@sjsu.edu}
        \and
        \IEEEauthorblockN{Hyelim Yang}
        \IEEEauthorblockA{\textit{Department of Artificial Intelligence} \\
        \textit{San José State University}\\
        San José, California \\
        hyelim.yang@sjsu.edu}
    }

    \maketitle

    \begin{abstract}
        
    Modern approaches to reinforcement learning (RL) are notorious for their extensive training times and complexity due, in part,
    to the necessary robustness of these systems in an uncertain environment space.\ As the problem space complexity continues to rise,
    more efficient systems must be developed to deal with the exponential rise in training times.\ DrQ-v2 is the second iteration of an
    off-policy actor-critic sampling algorithm which aims to efficiently model complex continuous systems with the help of data
    augmentation and notably uses single gpu architecture.\ DrQ-v2 focuses on solving visually continuous control through direct
    pixel observation.\ This state-of-the-art algorithm is the first model free approach to be capable of direct pixel observation
    and solve humanoid movement tasks in record time.\ In this paper we will highlight important components which contribute to its
    efficiency and ingenuity, then proceed to perform an ablation study.


    \end{abstract}

    \begin{IEEEkeywords}
        reinforcement learning, actor-critic, PPO
    \end{IEEEkeywords}

    \section{Introduction}\label{sec:introduction}
    %Example citation: DrQ-v2 paper~\cite{yarats2021drqv2}.

    %Example citation: original paper~\cite{yarats2021image}.

    %Example quotation: \enquote{This is important.}

    %Example inline math with vector notation: \(\mathbf{w} = [1, 2, 3]\).

    %Example math block:

    %\[w = w - \eta \nabla f(w).\]

    Efficient sampling of high-dimensional data has been a long standing problem in the field of RL.\ Through recent advances in
    sampling and data augmentation, DrQ-v2 has been able to solve complex humanoid tasks in a fraction of the time other models
    would need.\ DreamerV2 is one of these state-of-the-art models which has incorporated these recent advances.\ Despite this, it
    trains 4x slower than DrQ-v2 and requires a multi-gpu architecture.\ To understand this discrepancy in training times we must
    look at the design decisions and the components which make up the model.\ It uses a range of different methods to achieve its
    efficiency including a decay scheduler, sampling, improved replay buffer, and a deep deterministic policy gradient (DDPG).\
    The decay scheduler acts as a way of controlling the exploration rate of the model.\ Sampling is used to reduce the computational
    load by extracting 3 consecutive pixel observations at a time.\ This sampling efficiently models the policy by capturing the
    minimal amount of information necessary to make improvements.\ The replay buffer has been significantly updated by expanding
    its \ This increase has directly improved performance by 3.5x due to an increase in explored policy paths.\ Finally, and
    most notably, DrQ-v2 uses a DDPG to manage the rate of reward propagation.\ This process for continuous systems updates the
    critic by comparing the error from its network to that of the actor network error.\
    While this model is state-of-the-art, with a range of optimizations, there appears to be further room for improvement in a
    range of minor changes.\ The aim of this paper is thus to explore the inner mechanisms of DrQ-v2 and present additional optimizations
    that may incur performance benefits to our training time and convergence.\ We will also explore methods such as standard deviation
    tuning and learning rate scheduling to meet these goals.



    \section{Problem Definition}\label{sec:problem-definition}

    The stunning success of DrQ-v2, when compared to other state-of-the-art methods, warrants further exploration into its components.\
    During this initial exploration we found certain design decisions, such as hyperparameters and optimizers, which appeared to have
    been left unoptimized.\ With this understanding we began to formulate experiments we could conduct on DrQ-v2 which may offer some
    improvement to the base model.\ The following sections will demonstrate our attempts at improving training times and convergence
    rates given the already highly optimized design.\ We use San Jose State University’s HPC, which uses an Intel Xeon E5-2660 and an
    NVIDIA Tesla P100, to conduct our tests over 48 hour time intervals.\ Due to the computational efficiency of DrQ-v2, we expect this
    hardware to induce convergence.\

    \section{Methodology}\label{sec:methodology}

    DrQ-v2 has been demonstrated to achieve state-of-the-art results on a number of mujoco environments.\ As a result it is
    important to dissect the model apart and understand how it operates.\ In order to do this we must first explore the
    structure of the code.\ Once the code structure is understood, we will begin the ablation study by testing DrQ-v2 in
    multiple environments.\ We test a number of mujoco environments which are challenging enough to test the abilities of our
    model while maintaining the prospect of convergence within our 48 hour window.\ After selecting a promising environment,
    we will set a control run and tweak components in an iterative process.\ This will cleanly communicate if any adjustments
    to the model's components will present additional performance gains not present in the initial implementation of DrQ-v2.

    % Until Memory Issue Resolved (NOT SAME CODE AS ABOVE)
    %\iffalse
    \begin{tikzpicture}
    \begin{axis}[
        title = {Evaluating Tuning Hyperparameters on Walker Run for Reward by Epoch},
        xlabel = {Epoch},
        ylabel = {Reward},
        xmin = 0, xmax = 3100,
        ymin = 0, ymax = 770,
        xtick = {0,500,1000,1500,2000,2500,2500,3000,3500},
        ytick = {0,200,400,600,800,1000},
        legend pos = north west,
        ymajorgrids = true,
        grid = major,
        legend entries = {$A$, $B$, $C$, $D$, $E$},] % Hyper parameters for the plots below

    \addplot table [x = episode, y = episode_reward, col sep = comma]{walker_run_1520_eval_D_LRe-2.csv};
    \addplot table [x = episode, y = episode_reward, col sep = comma]{walker_run_3090_eval_D_LRe-3.csv};
    \addplot table [x = episode, y = episode_reward, col sep = comma]{walker_run_1550_eval_D.csv};
    \addplot table [x = episode, y = episode_reward, col sep = comma]{walker_run_2090_eval_D.csv};
    \addplot table [x = episode, y = episode_reward, col sep = comma]{walker_run_3090_eval_D.csv};

    \end{axis}
    \end{tikzpicture}

    \begin{tikzpicture}
    \begin{axis}[
        title = {Evaluating Varying Mujoco Environments using DrQ-v2},
        xlabel = {Epoch},
        ylabel = {Reward},
        xmin = 0, xmax = 3100,
        ymin = 0, ymax = 1200,
        xtick = {0,500,1000,1500,2000,2500,2500,3000,3500},
        ytick = {0,200,400,600,800,1000,1200},
        legend pos = north east,
        ymajorgrids = true,
        grid = major,
        legend entries = {$Cheetah$, $Human walk$, $Cartpole$},] % Hyper parameters for the plots below $walker run$}

    \addplot table [x = episode, y = episode_reward, col sep = comma]{cheeth_run_3090_eval_R.csv};
    \addplot table [x = episode, y = episode_reward, col sep = comma]{humanoid_walk_2420_eval_R.csv};
    \addplot table [x = episode, y = episode_reward, col sep = comma]{cartpole_balance_1090_eval_R.csv};
    %\addplot table [x = episode, y = episode_reward, col sep = comma]{walker_run_3090_train_D.csv};

    \end{axis}
    \end{tikzpicture}

    \begin{tikzpicture}
    \begin{axis}[
        title = {Training Tuning Hyperparameters on Walker Run for Reward by Epoch},
        xlabel = {Epoch},
        ylabel = {Reward},
        xmin = 0, xmax = 3100,
        ymin = 0, ymax = 770,
        xtick = {0,500,1000,1500,2000,2500,2500,3000,3500},
        ytick = {0,200,400,600,800,1000},
        legend pos = north west,
        ymajorgrids = true,
        grid = major,
        legend entries = {$.01$, $.001$, $C$, $D$, $E$},] % Hyper parameters for the plots below

    \addplot table [x = episode, y = episode_reward, col sep = comma]{walker_run_1520_train_D_LRe-2.csv};
    \addplot table [x = episode, y = episode_reward, col sep = comma]{walker_run_1520_train_D_LRe-3.csv};
    \addplot table [x = episode, y = episode_reward, col sep = comma]{walker_run_1550_train_D.csv};
    \addplot table [x = episode, y = episode_reward, col sep = comma]{walker_run_2090_train_D.csv};
    \addplot table [x = episode, y = episode_reward, col sep = comma]{walker_run_3090_train_D.csv};

    \end{axis}
    \end{tikzpicture}

    \begin{tikzpicture}
    \begin{axis}[
        title = {Training Varying Mujoco Environments using DrQ-v2},
        xlabel = {Epoch},
        ylabel = {Reward},
        xmin = 0, xmax = 3100,
        ymin = 0, ymax = 1200,
        xtick = {0,500,1000,1500,2000,2500,2500,3000,3500},
        ytick = {0,200,400,600,800,1000,1200},
        legend pos = north east,
        ymajorgrids = true,
        grid = major,
        legend entries = {$A$, $Human walk$, $Cartpole$, $walker run$},] % Hyper parameters for the plots below

    \addplot table [x = episode, y = actor_loss, col sep = comma]{cheeth_run_3090_train_R.csv};
    \addplot table [x = episode, y = actor_loss, col sep = comma]{humanoid_walk_2420_train_R.csv};
    \addplot table [x = episode, y = actor_loss, col sep = comma]{cartpole_balance_1090_train_R.csv};
    \addplot table [x = episode, y = actor_loss, col sep = comma]{walker_run_3090_train_R.csv};

    \end{axis}
    \end{tikzpicture}

    %\fi
    % fi ends the ignore statement for this section

    \section{Experiments}\label{sec:experiments}

    When studying the code and reading through the paper we noticed a few interesting points where optimization could be done.\
    Particularly we focused on the learning rate, the optimizer, and the standard deviation scheduler.\ These components were
    selected (due to the fact that each doesn't rely on another component).\ We conduct our experiment using the walker run mujoco
    environment.\ Starting with an initial control run we develop a baseline to improve our models from.\ We then tweak the
    hyperparameters in a multitude of ways in an attempt to build a stronger model.\ In the following sections we detail the theory
    and the results of these experiments.

    \section{Conclusion}\label{sec:results}

    TODO

    \section{Conclusion}\label{sec:conclusion}

    TODO

    \bibliography{main}
    \bibliographystyle{ieeetr}
\end{document}
