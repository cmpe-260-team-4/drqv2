%! suppress = EscapeUnderscore
%! Author = Andrew Selvia
%! Date = 2022.4.26

\documentclass[conference]{./IEEEtran/IEEEtran} % https://www.ctan.org/pkg/ieeetran
\usepackage{cite}
\usepackage[hidelinks]{hyperref}
\usepackage{csquotes} % https://tex.stackexchange.com/a/36813
\usepackage{mathtools} % https://tex.stackexchange.com/a/96353
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{tikz} % To generate the plot from csv

\usepackage{pgfplots}
\pgfplotsset{width = 10cm, compat = 1.9}

%% We will externalize the figures
\usepgfplotslibrary{external}
\tikzexternalize[prefix=figures/]


\begin{document}

    \title{Visual Continuous Control \\ {\large An Introduction to DrQ-v2 \& Ablations}}

    \author{
        \IEEEauthorblockN{Paul Mello}
        \IEEEauthorblockA{\textit{Department of Artificial Intelligence} \\
        \textit{San José State University}\\
        San José, California \\
        paul.mello@sjsu.edu}
        \and
        \IEEEauthorblockN{Andrew Selvia}
        \IEEEauthorblockA{\textit{Department of Software Engineering} \\
        \textit{San José State University}\\
        San José, California \\
        andrew.selvia@sjsu.edu}
        \and
        \IEEEauthorblockN{Hyelim Yang}
        \IEEEauthorblockA{\textit{Department of Artificial Intelligence} \\
        \textit{San José State University}\\
        San José, California \\
        hyelim.yang@sjsu.edu}
    }

    \maketitle

    \begin{abstract}
        Modern approaches to reinforcement learning (RL) are notorious for extensive training times and complexity.\ As
        they are applied to higher-dimensional problems, it is important to revisit their foundations with an eye for
        inefficiencies.\ That's exactly what the creators of one such algorithm (DrQ-v2) did.\ DrQ-v2~\cite{DrQv2} is
        the second iteration of the Data-regularized Q (DRQ) algorithm.\ It is an off-policy, actor-critic sampling
        algorithm.\ DrQ-v2 learns to control continuous environments via direct pixel observation.\ Notably, it is the
        first model-free algorithm \enquote{to solve complex humanoid locomotion tasks.}~\cite{DrQv2} This paper
        introduces the core concepts of DrQ-v2 and records experiments which ablate targeted areas of the core
        algorithm.
    \end{abstract}

    \begin{IEEEkeywords}
        DrQ-v2, Actor-Critic, Direct Pixel Observation, PPO
    \end{IEEEkeywords}

    \section{Introduction}\label{sec:introduction}
    Efficient sampling of high-dimensional data has been a long-standing problem in the field of RL.\ Through recent
    advances in sampling and data augmentation, DrQ-v2 has been able to solve complex humanoid tasks.
    DreamerV2~\cite{DreamerV2}, which slightly predates DrQ-v2, can also make this claim, though it is model-based.
    Critically, DrQ-v2 achieves the same efficacy in 4 times less training time than DreamerV2.\ The DrQ-v2 researchers
    use a range of methods to achieve their performance including modifying the degree of exploration during training,
    augmenting the data, expanding the replay buffer, and switching from a soft actor-critic (SAC) to a deep
    deterministic policy gradient (DDPG) design.

    %The exploration scheduler controls the rate of model variance between policies.\ Image data is augmented by
    %flipping, cropping, and padding to ensure generalization of the problem space.\ Sampling significantly reduces the computational
    %load by extracting three consecutive pixel observations at a time.\ This sampling efficiently models the policy without
    %the use of importance sampling.\ The replay buffer has been expanded to provide broader exposure to the environment.\
    %This increase has been directly attributed to a performance boost of 3.5x.\ Finally, and most notably, DrQ-v2 uses a DDPG
    %to manage the rate of reward propagation.\ This ensures DrQ-v2 remains stable as it progresses through the action space.\

    This study explores the inner mechanics of DrQ-v2 and injects various ablations intended to improve performance.
    These include the optimizer, learning rate, and exploration schedule.\ While these experiments did not yield any
    significant improvement to performance, they did aid in the authors' comprehension of its efficiency.

    \section{Methodology}\label{sec:methodology}
    DrQ-v2 has been demonstrated to achieve state-of-the-art results on a number of MuJoCo tasks.\ We test DrQ-v2 on
    many of these tasks including cheetah\_run, cartpole\_balance, and humanoid\_walk with the intent of finding a
    task that is challenging enough for experiments to have an impact while also converging within the constraints
    imposed by the system on which the experiments must run.\ All experiments were run on the San Jos\'e State
    University College of Engineering (CoE) High Performance Computing (HPC) system.\ The NVIDIA Tesla P100 GPUs
    provided by the HPC system were configured to run DrQ-v2 over the maximum allotted 48-hour time interval.
    Unfortunately, given the constraints of the HPC system, the landmark results on the humanoid\_walk task~\cite{DrQv2}
    were not reproducible.

    Preliminary exploration of the algorithm revealed specific hyperparameters which lent themselves to experimentation.
    The hyperparameters chosen include the optimizer, learning rate, and exploration schedule.\ Testing many tasks
    demonstrated that walker\_run struck the right balance between difficulty and convergence time as demonstrated in
    ~\ref{fig:1}.\ For example, the quadruped\_walk task does not converge within a reasonable amount of time.\ In
    contrast, the pendulum\_swingup task converges so quickly that no discernible effect is likely to be observed during
    an experiment.\ Thus, walker\_run was chosen as the control.\ Experiments were then conducted over the learning
    rate, optimizer, and exploration schedule in order to improve the performance attained by the hyperparameters
    published with the DrQ-v2 paper.

    \section{Experiments}\label{sec:experiments}
    The first experiment replaced the Adam optimizer with AdamW which is notable for its refinement of the original Adam
    with decoupled weight decay.\ This change weakened overall convergence.\ This discrepancy lies in AdamW being more
    selective in the policies it learns from.\ As a result, AdamW gained faster convergence for the first few hundred
    episodes.\ Despite this initial performance, the selectivity of AdamW becomes a detriment over time.\ However, given
    enough time, we would expect to find that AdamW converges to a better policy.

    The second experiment explored different values for the learning rate: $10^{-3}$ and $10^{-2}$.\ Additionally, an
    experiment was conducted using a dynamic learning rate.\ Unexpectedly, this experiment failed during training.\ Its
    results are omitted.
    
    The final experiment altered the exploration schedule.\ Manipulating this metric helps control the variation in
    explored policies based on the depth of the training.\ By default, DrQ-v2 uses a linear exploration schedule in
    which the standard deviation decreases from 1 to 0.1.\ The final experiment replaces the linear exploration schedule
    with a step\_linear function in which the standard deviation decreases linearly from 1 to 0.1 until episode 250,000;
    then, it resets the standard deviation to 1 and linearly decreases until the training completes.\ Figure~\ref{fig:2}
    demonstrates that the step\_linear schedule learns more rapidly and consistently than the linear one.
    
    \section{Results}\label{sec:results}
    Figure~\ref{fig:2} demonstrates that the experiments did not exceed the performance achieved with the published
    hyperparameters.\ However, the impact of the ablations is discernible.\ The first experiment which replaces Adam
    with AdamW tracks nearly identically to the control, though the reward it attains is consistently lower.\ Freed from
    the restrictions of the HPC system, we hypothesize that AdamW would eventually surpass Adam based on the slope its
    reward curve exhibits.\ AdamW is accelerating while Adam plateaus.

    The learning rate experiments proved to be largely inconsequential.\ Perhaps applied to a simpler task, they would
    have been more effective.

    The final experiment which replaces the linear exploration schedule with a step\_linear function outperforms the
    control briefly during the initial stages of learning before being overtaken.\ Close inspection of
    Figure~\ref{fig:2} reveals the accelerated exploration starting at episode 250 and the ensuing boost in reward.
    However, the reward curve plateaus early.\ Additional experimentation is required to acquire greater insight into
    why this happens.

    \begin{figure}[!ht]
        \begin{tikzpicture}
            \begin{axis}[
            scale = .8,
            title = {MuJoCo Environments: Reward},
            xlabel = {Episode},
            ylabel = {Reward},
            xmin = 0, xmax = 3200,
            ymin = 0, ymax = 1050,
            xtick = {0,800,1600,2400,3200},
            ytick = {0,150,300,450,600,750,900,1050},
            legend pos = south east,
            ymajorgrids = true,
            grid = major,
            mark size = .7pt,
            axis line style = ultra thin,
            legend style = {nodes = {scale = 0.75, transform shape}},
            legend entries = {walker\_run, cheetah\_run, walker\_walk, humanoid\_walk, quadruped\_walk,
                            walker\_stand, cartpole\_balance, pendulum\_swingup}
            ]
                \addplot [color = blue, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{experiments/203408-walker-run-eval-control.csv};
                \addplot [color = red, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{others/031508-cheetah-run-eval.csv};
                \addplot [color = cyan, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{others/154418-walker-walk-eval.csv};
                \addplot [color = gray, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{others/175732-humanoid-walk-eval.csv};
                \addplot [color = black, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{others/180549-quadruped-eval.csv};
                \addplot [color = orange, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{others/190918-walker-stand-eval.csv};
                \addplot [color = purple, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{others/191540-cartpole-balance-eval.csv};
                \addplot [color = green, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{others/201234-pendulum-swingup-eval.csv};
            \end{axis}
        \end{tikzpicture}
        \caption{A plot of the reward achieved on various MuJoCo tasks during each episode.}
        \label{fig:1}
    \end{figure}

    \begin{figure}[!ht]
        \begin{tikzpicture}
            \begin{axis}[
                scale = .8,
                title = {walker\_run Hyperparameter Tuning: Reward},
                xlabel = {Episode},
                ylabel = {Reward},
                xmin = 0, xmax = 3200,
                ymin = 0, ymax = 800,
                xtick = {0,800,1600,2400,3200},
                ytick = {0,200,400,600,800},
                legend pos = south east,
                ymajorgrids = true,
                grid = major,
                mark size = .7pt,
                axis line style = ultra thin,
                legend style = {nodes = {scale = 0.9, transform shape}},
                legend entries = {Control, Adamw, $\alpha = 10^{-2}$, $\alpha = 10^{-3}$, step\_linear}
            ]
                \addplot [color = blue, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{experiments/203408-walker-run-eval-control.csv};
                \addplot [color = orange, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{experiments/222844-walker-run-eval-Adamw.csv};
                \addplot [color = black, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{experiments/215829-walker-run-eval-1e-2.csv};
                \addplot [color = green, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{experiments/214858-walker-run-eval-1e-3.csv};
                \addplot [color = red, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{experiments/184403-walker-run-eval-stepliner.csv};
            \end{axis}
        \end{tikzpicture}
        \caption{A plot of the reward achieved on the walker\_ run task during the control and each of the conducted experiments.}
        \label{fig:2}
    \end{figure}

    \section{Conclusion}\label{sec:conclusion}

    This ablation study provided meaningful insight into the mechanisms that make DrQ-v2 a state-of-the-art RL
    algorithm.\ The experiments demonstrate that DrQ-v2 is a highly-efficient model with fine-tuned hyperparameters as
    evidenced by the difficulty we had in altering them.\ Despite this, future studies may focus on a multitude of
    components including the replay buffer and exploration schedule.

    \bibliographystyle{ieeetr}
    \bibliography{main}

\end{document}
