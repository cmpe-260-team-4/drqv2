%! suppress = EscapeUnderscore
%! Author = Andrew Selvia
%! Date = 2022.4.26

\documentclass[conference]{./IEEEtran/IEEEtran} % https://www.ctan.org/pkg/ieeetran
\usepackage{cite}
\usepackage[hidelinks]{hyperref}
\usepackage{csquotes} % https://tex.stackexchange.com/a/36813
\usepackage{mathtools} % https://tex.stackexchange.com/a/96353
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{tikz} % To generate the plot from csv

\usepackage{pgfplots}
\pgfplotsset{width = 10cm, compat = 1.9}

%% We will externalize the figures
\usepgfplotslibrary{external}
\tikzexternalize[prefix=figures/]

\begin{document}

    \title{DrQ-v2 Ablation Study}

    \author{
        \IEEEauthorblockN{Paul Mello}
        \IEEEauthorblockA{\textit{Department of Artificial Intelligence} \\
        \textit{San José State University}\\
        San José, California \\
        paul.mello@sjsu.edu}
        \and
        \IEEEauthorblockN{Andrew Selvia}
        \IEEEauthorblockA{\textit{Department of Software Engineering} \\
        \textit{San José State University}\\
        San José, California \\
        andrew.selvia@sjsu.edu}
        \and
        \IEEEauthorblockN{Hyelim Yang}
        \IEEEauthorblockA{\textit{Department of Artificial Intelligence} \\
        \textit{San José State University}\\
        San José, California \\
        hyelim.yang@sjsu.edu}
    }

    \maketitle

    \begin{abstract}
        
    Modern approaches to reinforcement learning (RL) are notorious for their extensive training times and complexity.\ The uncertain
    environment space necessitates the use of robust system design.\ As the complexity of the problem space rises, more efficient
    systems must be developed to improve training times. \ DrQ-v2~\cite{DrQv2} is the second iteration of an off-policy actor-critic
    sampling algorithm which aims to efficiently model complex continuous systems.\ This is done with the help of data augmentation
    techniques.\ DrQ-v2 focuses on solving visually continuous control through
    direct pixel observation.\ \enquote{Furthermore, DrQ-v2 is able to solve complex humanoid locomotion tasks directly from pixel
    observations, previously unattained by model-free RL.}~\cite{DrQv2} In this paper, we will highlight important components
    which contribute to the efficiency and ingenuity of the model.\ We will then demonstrate our experiments to measure the effectiveness of
    DrQ-v2's hyperparameters.\ Finally, we will present the results of our ablation study which demonstrate that DrQ-v2 is
    an exceptionally well-tuned model and one that may democratize the RL field.

    \end{abstract}

    \begin{IEEEkeywords}
        DrQ-v2, Actor-Critic, Pixel Observation, PPO
    \end{IEEEkeywords}

    \section{Introduction}\label{sec:introduction}


    %Example inline math with vector notation: \(\mathbf{w} = [1, 2, 3]\).
    %Example math block:
    %\[w = w - \eta \nabla f(w).\]

    Efficient sampling of high-dimensional data has been a long-standing problem in the field of RL.\ Through recent advances in
    sampling and data augmentation, DrQ-v2 has been able to solve complex humanoid tasks in a fraction of the time other models
    would need.\ DreamerV2~\cite{DreamerV2} is another recent state-of-the-art model which has incorporated these same recent advancements.
    Despite this, it trains 4x slower than DrQ-v2 and requires multiple GPUs.\ To understand this discrepancy in training
    times, we must study the design decisions and components.\ They use a range of methods to achieve their efficiency including
    modifying the degree of exploration during training, augmenting the data, expanding the size of the replay buffer,
    and switching from soft actor-critic (SAC) to deep deterministic policy gradient (DDPG) to improve reward shaping.
    The decay scheduler acts as a way of controlling the exploration rate within the model.\ Image data is augmented by
    flipping, cropping, and padding to ensure the model is properly generalizing the problem space.\ Sampling is done
    to reduce the computational load by extracting three consecutive pixel observations at a time.\ This sampling efficiently
    models the policy without the use of importance sampling. \ The replay buffer is expanded to provide broader exposure to the environment.
    This increase has directly improved performance by 3.5x due to increasing the explored policy paths.\ Finally, and most
    notably, DrQ-v2 uses a DDPG to manage the rate of reward propagation.\ This process updates the critic by comparing the error
    from its network to that of the actor network error.
    This study explores the inner mechanics of DrQ-v2 and injects various ablations intended to improve its performance.
    In particular, the optimizer, learning rate, and exploration noise are modified by experiments.\ These experiments
    did not yield any significant improvement to performance, though they did aid in the authors' comprehension of DrQ-v2.

    \section{Problem Definition}\label{sec:problem-definition}

    During our preliminary exploration we found certain hyperparameters which appear to offer potential performance benefits with fine tuning.\ With
    this understanding, we formulated potential experiments which could offer some improvement to the base model.\ The following sections will demonstrate our attempts at improving
    training times and convergence rates given the already highly optimized design of DrQ-v2.\ The experiments were all
    run on the San Jos\'e State University College of Engineering (CoE) High Performance Computing (HPC) system.
    The NVIDIA Tesla P100 GPUs provided by the HPC were configured to run DrQ-v2 over 48-hour time intervals.

    \section{Methodology}\label{sec:methodology}

    DrQ-v2 has been demonstrated to achieve state-of-the-art results on a number of MuJoCo environments.\ We test DrQ-v2 in multiple
    environments including cheetah\_run, cartpole\_balance, and humanoid\_walk.\ We test these MuJoCo environments to ensure they
    are challenging enough to test the abilities of our model while also maintaining the ability to converge within our 48-hour
    time limit.\ After selecting a promising environment, we executed a control run using the walker\_run environment.

    \section{Experiments}\label{sec:experiments}

    Experiments were conducted on the optimizer, learning rate, and exploration schedule.\ To begin our experiments, we must first find a MuJoCo
    environment which balances complexity and convergence.\ We then create a control test and attempt to improve from that policy.\
    We tweak the hyperparameters in an iterative process and record the results of the experiment.\ In the following section we
    detail these results.

    DrQ-v2 classifies MuJoCo environments in 3 difficulty categories: easy, medium, and hard.
    The hard environments proved too challenging to train on the HPC.\ The easy ones converged so quickly that
    experiments seemed unlikely to yield any observable advantage.\ Thus, the medium ones were explored to isolate a
    sufficiently-challenging task.\ Ultimately, the walker\_run environment was chosen as the control for the experiments.

    The DrQ-v2 paper records success in training hard tasks like humanoid\_walk.\ Unfortunately, given the constraints of
    the HPC, these results were not reproducible.~\ref{fig:1} demonstrates the rewards gained by each environment we
    tested while~\ref{fig:2} demonstrates the actor loss.

    \begin{figure}[!ht]
        \begin{tikzpicture}
            \begin{axis}[
            scale = .8,
            title = {Mujoco Environments: Reward},
            xlabel = {Episode},
            ylabel = {Reward},
            xmin = 0, xmax = 3200,
            ymin = 0, ymax = 1050,
            xtick = {0,800,1600,2400,3200},
            ytick = {0,150,300,450,600,750,900,1050},
            legend pos = south east,
            ymajorgrids = true,
            grid = major,
            mark size = .7pt,
            axis line style = ultra thin,
            legend style = {nodes = {scale = 0.75, transform shape}},
            legend entries = {$Walker Run$, $Cheetah$, $Walker Walk$, $Humanoid Walk$, $Quadruped$,
                            $Walker Stand$, $Cartpole Balance$, $Pendulum Swingup$}
            ]

        \addplot [color = blue, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{experiments/203408-walker-run-eval-control.csv};
        \addplot [color = red, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{others/031508-cheetah-run-eval.csv};
        \addplot [color = cyan, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{others/154418-walker-walk-eval.csv};
        \addplot [color = gray, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{others/175732-humanoid-walk-eval.csv};
        \addplot [color = black, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{others/180549-quadruped-eval.csv};
        \addplot [color = orange, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{others/190918-walker-stand-eval.csv};
        \addplot [color = purple, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{others/191540-cartpole-balance-eval.csv};
        \addplot [color = green, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{others/201234-pendulum-swingup-eval.csv};

        \end{axis}
        \end{tikzpicture}
        \label{fig:1}
    \end{figure}

    \begin{figure}[!ht]
        \begin{tikzpicture}
        \begin{axis}[
            scale = .8,
            title = {Mujoco Environments: Actor Loss},
            xlabel = {Episode},
            ylabel = {Actor Loss},
            xmin = 0, xmax = 3200,
            ymin = -200, ymax = 0,
            xtick = {0,800,1600,2400,3200},
            ytick = {0,-25,-50,-75,-100,-125,-150,-175,-200},
            legend pos = north east,
            ymajorgrids = true,
            grid = major,
            mark size = .5pt,
            axis line style = ultra thin,
            legend style = {nodes = {scale = 0.75, transform shape}},
            legend entries = {$Walker Run$, $Cheetah$, $Walker Walk$, $Humanoid Walk$, $Quadruped$,
                            $Walker Stand$, $Cartpole Balance$, $Pendulum Swingup$}
        ]

        \addplot [color = blue, smooth, ultra thick] table [x = episode, y = actor_loss, col sep = comma]{experiments/203408-walker-run-train-control.csv};
        \addplot [color = red, smooth, ultra thick] table [x = episode, y = actor_loss, col sep = comma]{others/031508-cheetah-run-train.csv};
        \addplot [color = cyan, smooth, ultra thick] table [x = episode, y = actor_loss, col sep = comma]{others/154418-walker-walk-train.csv};
        \addplot [color = gray, smooth, ultra thick] table [x = episode, y = actor_loss, col sep = comma]{others/175732-humanoid-walk-train.csv};
        \addplot [color = black, smooth, ultra thick] table [x = episode, y = actor_loss, col sep = comma]{others/180549-quadruped-train.csv};
        \addplot [color = orange, smooth, ultra thick] table [x = episode, y = actor_loss, col sep = comma]{others/190918-walker-stand-train.csv};
        \addplot [color = purple, smooth, ultra thick] table [x = episode, y = actor_loss, col sep = comma]{others/191540-cartpole-balance-train.csv};
        \addplot [color = green, smooth, ultra thick] table [x = episode, y = actor_loss, col sep = comma]{others/201234-pendulum-swingup-train.csv};

        \end{axis}
        \end{tikzpicture}
        \label{fig:2}
    \end{figure}

    ~\ref{fig:1} and~\ref{fig:2} both demonstrate that walker\_run is the best balance between convergence and complexity.\ Other environments,
    such as cheetah\_run and pendulum\_swingup converge too rapidly or not rapidly enough.\ This would prevent adequate visualization
    regarding the performance of our hyperparameter manipulations.\ We test the strength of our hyperparameter adjusments using
    the walker\_run environment.\ We perform our ablation study by manipulating the learning rates, optimizers, and standard
    deviation schedulers in an attempt to improve on the hyper parameters of the control run.\

    \section{Results}\label{sec:results}

    % Figure 3
    \begin{figure}[!ht]
        \begin{tikzpicture}
        \begin{axis}[
            scale = .8,
            title = {Walker Run Hyperparameter Tuning: Reward},
            xlabel = {Episode},
            ylabel = {Reward},
            xmin = 0, xmax = 3200,
            ymin = 0, ymax = 800,
            xtick = {0,800,1600,2400,3200},
            ytick = {0,200,400,600,800},
            legend pos = south east,
            ymajorgrids = true,
            grid = major,
            mark size = .7pt,
            axis line style = ultra thin,
            legend style = {nodes = {scale = 0.9, transform shape}},
            legend entries = {$Control$, $Step-Linear$, $Lr:1e-2$, $Lr:1e-3$, $Adamw$}
        ]

        \addplot [color = blue, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{experiments/203408-walker-run-eval-control.csv};
        \addplot [color = red, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{experiments/184403-walker-run-eval-stepliner.csv};
        \addplot [color = black, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{experiments/215829-walker-run-eval-1e-2.csv};
        \addplot [color = green, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{experiments/214858-walker-run-eval-1e-3.csv};
        \addplot [color = orange, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{experiments/222844-walker-run-eval-Adamw.csv};

        \end{axis}
        \end{tikzpicture}
        \caption{}\label{fig:3}
    \end{figure}

    % Figure 4
    \begin{figure}[!ht]
        \begin{tikzpicture}
        \begin{axis}[
            scale = .75,
            title = {Walker Run Hyperparameter Tuning: Actor Loss},
            xlabel = {Episode},
            ylabel = {Actor Loss},
            xmin = 0, xmax = 3200,
            ymin = -165, ymax = 0,
            xtick = {0,800,1600,2400,3200},
            ytick = {0,-25,-50,-75,-100,-125,-150,-175},
            legend pos = north east,
            ymajorgrids = true,
            grid = major,
            mark size = .5pt,
            axis line style = ultra thin,
            legend style = {nodes = {scale = 0.9, transform shape}},
            legend entries = {$Control$, $StepLinear$, $Lr:1e-2$, $Lr:1e-3$, $Adamw$}
        ]

        \addplot [color = blue, smooth, ultra thick] table [x = episode, y = actor_loss, col sep = comma]{experiments/203408-walker-run-train-control.csv};
        \addplot [color = red, smooth, ultra thick] table [x = episode, y = actor_loss, col sep = comma]{experiments/184403-walker-run-train-stepliner.csv};
        \addplot [color = black, smooth, ultra thick] table [x = episode, y = actor_loss, col sep = comma]{experiments/215829-walker-run-train-1e-2.csv};
        \addplot [color = green, smooth, ultra thick] table [x = episode, y = actor_loss, col sep = comma]{experiments/214858-walker-run-train-1e-3.csv};
        \addplot [color = orange, smooth, ultra thick] table [x = episode, y = actor_loss, col sep = comma]{experiments/222844-walker-run-train-Adamw.csv};

        \end{axis}
        \end{tikzpicture}
        \label{fig:4}
    \end{figure}

    ~\ref{fig:3} and~\ref{fig:4} demonstrate this ablation study did not result in better performance than the initial control model.\ We find
    that increasing the learning rate destroyed the tractability of our model as it was unable to learn during its training.\
    We test a total of three learning rates.\ A learning rate schedule was also explored, but it never functioned properly.
    DrQ-v2 crashes shortly after training starts when a learning rate schedule is used.

    With respect to the optimizer, we pivoted from Adam to AdamW.\ We found that this change produced slightly worse results than
    the control runs using Adam.\ We believe this discrepancy lies in AdamW being more robust and thus more conservative in its
    approach to learning.\ This results in AdamW gaining faster convergence for the first few hundred runs and inevitably causes
    it to fall behind as we continue to improve the policy.\ However, with enough time, we would expect to find that AdamW converges
    to the same policy as the control run.

    The final experiment altered the exploration schedule.\ TODO: discuss difference between linear and step\_linear.
    TODO: rewrite results.

    We find that some of our experiments were able to converge to a nearly optimal policy quicker than that of the control run.\
    However, as training continues, the hyperparameters proposed by the DrQ-v2 researchers outperform our experiments.

    \section{Conclusion}\label{sec:conclusion}

    This ablation study provided meaningful insight into the mechanisms that make DrQ-v2 a state-of-the-art RL algorithm.
    Future studies......... TODO: write more.

    \bibliographystyle{ieeetr}
    \bibliography{main}

\end{document}
