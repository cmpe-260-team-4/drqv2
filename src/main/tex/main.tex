%! suppress = EscapeUnderscore
%! Author = Andrew Selvia
%! Date = 2022.4.26

\documentclass[conference]{./IEEEtran/IEEEtran} % https://www.ctan.org/pkg/ieeetran
\usepackage{cite}
\usepackage[hidelinks]{hyperref}
\usepackage{csquotes} % https://tex.stackexchange.com/a/36813
\usepackage{mathtools} % https://tex.stackexchange.com/a/96353
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{tikz} % To generate the plot from csv

\usepackage{pgfplots}
\pgfplotsset{width = 10cm, compat = 1.9}

%% We will externalize the figures
\usepgfplotslibrary{external}
\tikzexternalize[prefix=figures/]


\begin{document}

    \title{Visual Continuous Control \\ {\large An Introduction to DrQ-v2 \& Ablations}}

    \author{
        \IEEEauthorblockN{Paul Mello}
        \IEEEauthorblockA{\textit{Department of Artificial Intelligence} \\
        \textit{San José State University}\\
        San José, California \\
        paul.mello@sjsu.edu}
        \and
        \IEEEauthorblockN{Andrew Selvia}
        \IEEEauthorblockA{\textit{Department of Software Engineering} \\
        \textit{San José State University}\\
        San José, California \\
        andrew.selvia@sjsu.edu}
        \and
        \IEEEauthorblockN{Hyelim Yang}
        \IEEEauthorblockA{\textit{Department of Artificial Intelligence} \\
        \textit{San José State University}\\
        San José, California \\
        hyelim.yang@sjsu.edu}
    }

    \maketitle

    \begin{abstract}
        Modern approaches to reinforcement learning (RL) are notorious for extensive training times and complexity.\ As
        they are applied to higher-dimensional problems, it is important to revisit their foundations with an eye for
        inefficiencies.\ That's exactly what the creators of one such algorithm (DrQ-v2) did.\ DrQ-v2~\cite{DrQv2} is
        the second iteration of an off-policy, actor-critic sampling algorithm.\ DrQ-v2 learns to control continuous
        environments via direct pixel observation.\ Notably, it is the first model-free algorithm \enquote{to solve
        complex humanoid locomotion tasks.}~\cite{DrQv2} This paper introduces the core concepts of DrQ-v2 and records
        experiments which ablate targeted areas of the core algorithm.
    \end{abstract}

    \begin{IEEEkeywords}
        DrQ-v2, Actor-Critic, Direct Pixel Observation, PPO
    \end{IEEEkeywords}

    \section{Introduction}\label{sec:introduction}
    Efficient sampling of high-dimensional data has been a long-standing problem in the field of RL.\ Through recent
    advances in sampling and data augmentation, DrQ-v2 has been able to solve complex humanoid tasks.
    DreamerV2~\cite{DreamerV2}, which slightly predates DrQ-v2, can also make this claim, though it is model-based.
    Critically, DrQ-v2 achieves the same efficacy in 4 times less training time than DreamerV2.\ The DrQ-v2 researchers
    use a range of methods to achieve their performance including modifying the degree of exploration during training,
    augmenting the data, expanding the replay buffer, and switching from a soft actor-critic (SAC) to a deep
    deterministic policy gradient (DDPG) design.

    %The exploration scheduler controls the rate of model variance between policies.\ Image data is augmented by
    %flipping, cropping, and padding to ensure generalization of the problem space.\ Sampling significantly reduces the computational
    %load by extracting three consecutive pixel observations at a time.\ This sampling efficiently models the policy without
    %the use of importance sampling.\ The replay buffer has been expanded to provide broader exposure to the environment.\
    %This increase has been directly attributed to a performance boost of 3.5x.\ Finally, and most notably, DrQ-v2 uses a DDPG
    %to manage the rate of reward propagation.\ This ensures DrQ-v2 remains stable as it progresses through the action space.\

    This study explores the inner mechanics of DrQ-v2 and injects various ablations intended to improve performance.
    These include the optimizer, learning rate, and exploration noise.\ While these experiments did not yield any
    significant improvement to performance, they did aid in the authors' comprehension of its efficiency.

    \section{Methodology}\label{sec:methodology}

    Preliminary exploration demonstrated certain hyperparameters to be susceptible to significant optimizations.\ The following
    sections will demonstrate our ablations to improve training times and or convergence rates given the already highly optimized
    design of DrQ-v2.\ All experiments were run on the San Jos\'e State University College of Engineering (CoE) High Performance
    Computing (HPC) system.\ The NVIDIA Tesla P100 GPUs provided by the HPC were configured to run DrQ-v2 over 48-hour time intervals.

    DrQ-v2 has been demonstrated to achieve state-of-the-art results on a number of MuJoCo environments.\ We test DrQ-v2 in multiple
    environments including cheetah\_run, cartpole\_balance, and humanoid\_walk.\ Testing multiple environments ensures we select a
    control that is challenging enough to test the abilities of DrQ-v2 while also maintaining convergence capabilities within
    our 48-hour time limit.\ Once an environment has been selected various ablations will be conducted and results will be
    gathered for visualization.\

    \section{Experiments}\label{sec:experiments}

    Experiments were conducted on the optimizer, learning rate, and exploration schedule.\ Our resource restrictions require a
    MuJoCo environment which balances complexity and convergence.\ Testing many environments demonstrated that walker\_run balanced
    these metrics efficiently.\ A control was created using walker\_run and Hyperparameter tests were conducted to independently
    ensure each change could be attributable to a specific component.\

    The DrQ-v2 paper records success in training difficult tasks like humanoid\_walk.\ Unfortunately, given the constraints of
    the HPC, these results were not reproducible.\ Figure~\ref{fig:1} demonstrates the rewards gained by each environment
    while Figure~\ref{fig:2} demonstrates the actor loss.

    \begin{figure}[!ht]
        \begin{tikzpicture}
            \begin{axis}[
            scale = .8,
            title = {MuJoCo Environments: Reward},
            xlabel = {Episode},
            ylabel = {Reward},
            xmin = 0, xmax = 3200,
            ymin = 0, ymax = 1050,
            xtick = {0,800,1600,2400,3200},
            ytick = {0,150,300,450,600,750,900,1050},
            legend pos = south east,
            ymajorgrids = true,
            grid = major,
            mark size = .7pt,
            axis line style = ultra thin,
            legend style = {nodes = {scale = 0.75, transform shape}},
            legend entries = {walker\_run, cheetah\_run, walker\_walk, humanoid\_walk, quadruped\_walk,
                            walker\_stand, cartpole\_balance, pendulum\_swingup}
            ]
                \addplot [color = blue, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{experiments/203408-walker-run-eval-control.csv};
                \addplot [color = red, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{others/031508-cheetah-run-eval.csv};
                \addplot [color = cyan, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{others/154418-walker-walk-eval.csv};
                \addplot [color = gray, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{others/175732-humanoid-walk-eval.csv};
                \addplot [color = black, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{others/180549-quadruped-eval.csv};
                \addplot [color = orange, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{others/190918-walker-stand-eval.csv};
                \addplot [color = purple, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{others/191540-cartpole-balance-eval.csv};
                \addplot [color = green, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{others/201234-pendulum-swingup-eval.csv};
            \end{axis}
        \end{tikzpicture}
        \caption{A plot of the reward achieved on various MuJoCo tasks during each episode.}
        \label{fig:1}
    \end{figure}

    Figure~\ref{fig:1} demonstrates that walker\_run is the best balance between convergence and complexity.\ Other environments,
    such as cheetah\_run and pendulum\_swingup converge too rapidly or not rapidly enough.\ This would prevent adequate visualization
    regarding the performance of our hyperparameter manipulations.\ We study the learning rates, optimizers, and standard
    deviation scheduler in order to improve the proposed hyperparameters published within the DrQ-v2 paper.\

    \section{Results}\label{sec:results}

    \begin{figure}[!ht]
        \begin{tikzpicture}
            \begin{axis}[
                scale = .8,
                title = {walker\_run Hyperparameter Tuning: Reward},
                xlabel = {Episode},
                ylabel = {Reward},
                xmin = 0, xmax = 3200,
                ymin = 0, ymax = 800,
                xtick = {0,800,1600,2400,3200},
                ytick = {0,200,400,600,800},
                legend pos = south east,
                ymajorgrids = true,
                grid = major,
                mark size = .7pt,
                axis line style = ultra thin,
                legend style = {nodes = {scale = 0.9, transform shape}},
                legend entries = {Control, Adamw, $\alpha = 10^{-2}$, $\alpha = 10^{-3}$, step\_linear}
            ]
                \addplot [color = blue, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{experiments/203408-walker-run-eval-control.csv};
                \addplot [color = orange, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{experiments/222844-walker-run-eval-Adamw.csv};
                \addplot [color = black, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{experiments/215829-walker-run-eval-1e-2.csv};
                \addplot [color = green, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{experiments/214858-walker-run-eval-1e-3.csv};
                \addplot [color = red, smooth, ultra thick] table [x = episode, y = episode_reward, col sep = comma]{experiments/184403-walker-run-eval-stepliner.csv};
            \end{axis}
        \end{tikzpicture}
        \caption{A plot of the reward achieved on the walker\_ run task during the control and each of the conducted experiments.}
        \label{fig:3}
    \end{figure}

    Figure~\ref{fig:3} demonstrates the ablation study did not result in better performance than the published
    hyperparameters.\ Two large learning rates were tested and a learning rate scheduler was introduced.\ Unexpectedly, the scheduler
    failed during training and the bigger learning destroyed the tractability.\ Using a smaller learning rate may find prove to
    improve performance.\

    Another hyperparameter adjustment was introduced by replacing Adam with AdamW.\ We found that this change weakened overall
    convergence .\ We believe this discrepancy lies in AdamW being more selective in the policies it learns from.\ As a result
    AdamW gained faster convergence for the first few hundred runs.\ Despite this initial performance, the selectivity of AdamW
    becomes a detriment as Adam quickly surpasses it.\ However, given enough time, we would expect to find that AdamW converges
    to a better policy.\

    Our final ablation altered the exploration schedule.\ Manipulating this metric helps control the variation in explored policies
    based on the depth of the model.\ Starting from 1, DrQ-v2 uses a linearly declining function that clips the standard deviation
    at .1 after many steps.\ Step\_linear introduces additional steps which we can further augment the standard deviation.\ Experiments
    demonstrated this metric directly affects convergence rates.\ Figure~\ref{fig:3} demonstrates that step\_linear learns quicker
    and more consistently than the linear alternative.\

    We find that some of our experiments were able to converge to a nearly optimal policy quicker than that of the control run.
    However, as training continues, the hyperparameters proposed by the DrQ-v2 researchers outperform our experiments.\ The efficiency
    of DrQ-v2 leaves little room for general model improvements, but environment specific adjustments appear to be plausible.

    %TODO: rewrite results.


    \section{Conclusion}\label{sec:conclusion}

    This ablation study provided meaningful insight into the mechanisms that make DrQ-v2 a state-of-the-art RL algorithm.\ Experiments
    demonstrated that DrQ-v2 is a highly efficient model with fine-tuned hyperparameters.\ Despite this, future studies may
    focus on a multitude of components including the replay buffer, exploration schedule, and introducing a learning rate scheduler.

    %Future studies......... TODO: write more.

    \bibliographystyle{ieeetr}
    \bibliography{main}

\end{document}
